{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificacion de archivo para carga incremental de la tabla review\n",
    "\n",
    "Preparacion para prueba:\n",
    "Se utilizara el archivo 'review.json' para sacar tres conjuntos de dataset y se crearan 2 mas:\n",
    "    1. 'reviews_inicial.json'     , Archivo con la mayoria de la data chunk 14/17     \n",
    "    2. 'reviews_incremental.json' , Archivo con datos en perfecto estado chunk 2/17\n",
    "    3. 'reviews_incremental_con_repeticiones.json' , archivo con datos chunk 1/17 y con datos \n",
    "        repetidos de chunks anteriores.\n",
    "    4. 'reviews_incremental_con_fallas_columnas.json', Archivo .json con columnas aleatorio\n",
    "    5. 'reviews_incremental_con fallas_formato_datos.json', Archivo .json con columnas que coinciden \n",
    "        con 'reviews_inicial.json' pero su contenido no.\n",
    "    \n",
    "nota: no es necesario tomar en cuenta la fecha del review para clasificar\n",
    "los datos, es preferible datos aleatorios, los chunks que se crean \n",
    "cuando se crea  'reviews_inicial.parquet.gzip'\n",
    "\n",
    "Procedimiento de verificacion (Bussnes Rules):\n",
    "Al introducir un nuevo archivo .json se verifica en formato:\n",
    "    1. cantidad de columnas\n",
    "    2. nombre de columnas\n",
    "    3. formato de datos en columnas\n",
    "Si cumple con formato se verifica contenido, se trabaja con los codigos alfanumericos \n",
    "del nuevo dato/review review_id,user_id y business_id:\n",
    "\n",
    "    4. tabla 'idreview[id_review(int),review_id(tex)]';\n",
    "        4.1. si el review_id ya existen en tabla base \n",
    "            se descarta el dato/review nuevo\n",
    "        4.2. de lo contrario\n",
    "            se revisan los puntos 5.1 y 6.1\n",
    "            se incorpora el nuevo review a la tabla reviews y a la tabla idreviews\n",
    "\n",
    "    5. tabla idusers[id_user(int),user_id(tex)], \n",
    "        5.1 si el user_id NO esta en la tabla 'idusers' \n",
    "            Se descarta dato/review nuevo\n",
    "    nota: un procedimento mas correcto seria, agrega el nuevo user a la tabla 'users' para lo cual se necesitarian los datos [review_count, yelping_since, .... average_stars], al actualizar esa data automatiacamente se actualiza la tabla 'idusers' y despues se procede con el punto 4.2\n",
    "\n",
    "\n",
    "    6. tabla 'idbusiness[id_business(int),business_id(tex)], \n",
    "        6.1 si el business_id del nuevo data/review NO esta en la tabla 'idbusiness'\n",
    "            Se descarta dato/review nuevo\n",
    "    nota: un procedimento mas correcto seria, agrega el nuevo busines a la tabla 'business' para lo cual se necesitarian los datos [name, address, .... categories_id], adicionalmente se debe verifiar su categoria,\n",
    "    si coincide con las categorias a usar se agrega a la tabla  'business', y automaticamente actualizar las tablas 'idbusiness', 'business_attributes', 'business_categories', 'business_city_state', 'business_hours' para luego proceder con el punto 4.2\n",
    "\n",
    "\n",
    "Reqerimientos:\n",
    "    1. Se requiere acceder a la tabla idreviews \n",
    "    2. Se requiere  acceder a la tabla idbussines \n",
    "    3. Se requiere  acceder a la tabla idusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condiciones de incio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a utilizar\n",
    "import numpy \n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Direccion incial donde estan archivos de entrada, y direccion de archivos salida\n",
    "path_incial = 'C:\\\\Github\\\\DataSet_YELP\\\\'\n",
    "path_final = 'C:\\\\Github\\\\DataSet_YELP\\\\pruebas_incremental\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado carga: 35/35  100.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cont = 0\n",
    "#Se sabe que tamaño +200k  se haran \n",
    "# 35 partes (chunk) del .json este \n",
    "# estimado es solo para \"reviews.json\"\n",
    "cont_max = 35  \n",
    "Tamano = 200000\n",
    "for chunk in pd.read_json(path_incial+'review.json',chunksize=Tamano, lines=True):\n",
    "    # Visualizacion de avance\n",
    "    cont = cont + 1\n",
    "    clear_output(wait=True)\n",
    "    print('Completado carga: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    #Se carga cada parte\n",
    "    if cont == 1:\n",
    "        review = chunk\n",
    "    else:\n",
    "        review = pd.concat([review, chunk], ignore_index=True)\n",
    "        \n",
    "#tiempo = 7 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arbitrariamente se define:  \n",
    "#   70% de data en 'reviews_inicial.json' , \n",
    "#   20% en 'reviews_incremental.json' y \n",
    "#   10% en 'reviews_incremental_con_repeticiones.json'\n",
    "\n",
    "#creacion de archivo 'reviews_inicial.json' \n",
    "corte_0 = int(round(review.shape[0]*0.7,0))\n",
    "reviews_inicial = review.iloc[:corte_0,:]\n",
    "reviews_inicial.to_json(path_final+'reviews_inicial.json')\n",
    "\n",
    "#creacion de archivo 'reviews_incremental.json' \n",
    "corte_1 = int(round(review.shape[0]*0.9,0))\n",
    "reviews_incremental = review.iloc[corte_0+1:corte_1,:]\n",
    "reviews_incremental.to_json(path_final+'reviews_incremental.json')\n",
    "\n",
    "#creacion de archivo 'reviews_incremental_con_repeticiones.json' \n",
    "df = review.iloc[corte_1+1:,:]\n",
    "df2 = reviews_incremental.sample(200000)\n",
    "review_repeticiones = pd.concat([df, df2], ignore_index=True)\n",
    "review_repeticiones.to_json(path_final+'reviews_incremental_con_repeticiones.json')\n",
    "\n",
    "#creacion de archivo .json con columnas aleatorio\n",
    "df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "df.to_json(path_final+'reviews_aleatorio1.json')\n",
    "\n",
    "#creacion de archivo .json con columnas correctas pero datos aleatorio\n",
    "columnas = ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny','cool', 'text', 'date']\n",
    "df = pd.DataFrame(np.random.randn(1000, 9), columns=columnas)\n",
    "df.to_json(path_final+'reviews_aleatorio2.json')\n",
    "\n",
    "del review, columnas, corte_1, corte_0, df, df2, reviews_inicial, reviews_incremental, review_repeticiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedimiento de verificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archivo_en_verificacion = 'reviews_aleatorio2.json'\n",
    "#archivo_en_verificacion = 'reviews_aleatorio1.json'\n",
    "#archivo_en_verificacion = 'reviews_incremental.json'\n",
    "archivo_en_verificacion =  'reviews_incremental_con_repeticiones.json'\n",
    "path = 'C:\\\\Github\\\\DataSet_YELP\\\\pruebas_incremental\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_json(path+archivo_en_verificacion)\n",
    "\n",
    "#Mejor seria cargar inicialmente la primera fila y \n",
    "# despues todo el archivo pero como son pequeños no hay problema en hacerlo al reves \n",
    "review_prueba = df.iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condicion incial\n",
    "check_03_Columnas = False\n",
    "\n",
    "# revision cantidad columnas\n",
    "check_01_Columnas = len(review_prueba.columns) == 9\n",
    "\n",
    "#nombre de columnas\n",
    "if check_01_Columnas:\n",
    "    L1 = ['review_id', 'user_id', 'business_id', 'useful','stars',  'funny','cool', 'text', 'date']\n",
    "    L2 = list(review_prueba.columns)\n",
    "    check_02_Columnas = len([i for i in L1 if i in L2]) == 9\n",
    "\n",
    "#formato de datos en columnas\n",
    "if check_02_Columnas:\n",
    "    def chequea_formato_columnas(review_prueba):\n",
    "        resultado = True\n",
    "        resultado = isinstance(review_prueba['review_id'].loc[0], str) & resultado\n",
    "        resultado = isinstance(review_prueba['user_id'].loc[0], str) & resultado\n",
    "        resultado = isinstance(review_prueba['business_id'].loc[0], str) & resultado\n",
    "        resultado = isinstance(review_prueba['stars'].loc[0], numpy.int64) & resultado\n",
    "        resultado = isinstance(review_prueba['useful'].loc[0], numpy.int64) & resultado\n",
    "        resultado = isinstance(review_prueba['funny'].loc[0], numpy.int64) & resultado\n",
    "        resultado = isinstance(review_prueba['text'].loc[0], str) & resultado\n",
    "        resultado = isinstance(review_prueba['date'].loc[0], pandas._libs.tslibs.timestamps.Timestamp)  & resultado\n",
    "        return resultado\n",
    "    check_03_Columnas = chequea_formato_columnas(review_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexion para carga de tablas Id desde  base de datos local\n",
    "conexion = pymysql.connect(\n",
    "   host = '127.0.0.1',\n",
    "   user = 'root',\n",
    "   passwd='JCSR.MySQL123',\n",
    "   db='proyecto_yelp_24_11_22'\n",
    ")\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "#conexion a googlecloud\n",
    "conexionGooglecloud = pymysql.connect(\n",
    "   #host = '177.226.145.197',  #para revisar si conecta\n",
    "   host = '34.176.218.33',\n",
    "   user = 'root',\n",
    "   passwd='projectyelp2022',\n",
    "   db='prueba'\n",
    ")\n",
    "cursorcloud = conexionGooglecloud.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"show tables;\")\n",
    "\n",
    "Resultado = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('business',),\n",
       " ('business_attributes',),\n",
       " ('business_categories',),\n",
       " ('business_city_state',),\n",
       " ('business_hours',),\n",
       " ('idbusiness',),\n",
       " ('iduser',),\n",
       " ('reviews',),\n",
       " ('reviews2',),\n",
       " ('users',))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_03_Columnas:\n",
    "    #Carga de tabla id reviews desde la base de datos\n",
    "    \n",
    "    \n",
    "    reviews_inicial.parquet.gzip\n",
    "    pass\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedimiento de verificacion (Bussnes Rules):\n",
    "Al introducir un nuevo archivo .json se verifica en formato:\n",
    "    1. cantidad de columnas\n",
    "    2. nombre de columnas\n",
    "    3. formato de datos en columnas\n",
    "Si cumple con formato se verifica contenido, se trabaja con los codigos alfanumericos \n",
    "del nuevo dato/review review_id,user_id y business_id:\n",
    "\n",
    "    4. tabla 'idreview[id_review(int),review_id(tex)]';\n",
    "        4.1. si el review_id ya existen en tabla base \n",
    "            se descarta el dato/review nuevo\n",
    "        4.2. de lo contrario\n",
    "            se revisan los puntos 5.1 y 6.1\n",
    "            se incorpora el nuevo review a la tabla reviews y a la tabla idreviews\n",
    "\n",
    "    5. tabla idusers[id_user(int),user_id(tex)], \n",
    "        5.1 si el user_id NO esta en la tabla 'idusers' \n",
    "            Se descarta dato/review nuevo\n",
    "    nota: un procedimento mas correcto seria, agrega el nuevo user a la tabla 'users' para lo cual se necesitarian los datos [review_count, yelping_since, .... average_stars], al actualizar esa data automatiacamente se actualiza la tabla 'idusers' y despues se procede con el punto 4.2\n",
    "\n",
    "\n",
    "    6. tabla 'idbusiness[id_business(int),business_id(tex)], \n",
    "        6.1 si el business_id del nuevo data/review NO esta en la tabla 'idbusiness'\n",
    "            Se descarta dato/review nuevo\n",
    "    nota: un procedimento mas correcto seria, agrega el nuevo busines a la tabla 'business' para lo cual se necesitarian los datos [name, address, .... categories_id], adicionalmente se debe verifiar su categoria,\n",
    "    si coincide con las categorias a usar se agrega a la tabla  'business', y automaticamente actualizar las tablas 'idbusiness', 'business_attributes', 'business_categories', 'business_city_state', 'business_hours' para luego proceder con el punto 4.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a5f5e9430e63759269e8e709c4002b1ad533978ca32d2fcf985e534411cec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
