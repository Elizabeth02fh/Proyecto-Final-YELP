{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particion de dataset para carga incremental\n",
    "STORY:\n",
    "Los archivos son muy grandes por lo que se debe hacer la carga por partes en la base de datos Mysql, para lo\n",
    "cual se prepara este script, adicionalmente para probar el escrip de carga incremental se modifica la data creando \n",
    "valores repetidos y archivos cuyo contenido no coincide con el formato esperado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Condiciones de incio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a utilizar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Direccion incial donde estan archivos de entrada y donde se almacenaran archivos salida\n",
    "path_incial = 'C:\\\\Github\\\\DataSet_YELP\\\\'\n",
    "path_final = 'C:\\\\Github\\\\DataSet_YELP\\\\pruebas_incremental\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Creacion de datos prueba para review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de archivo review en tabla review\n",
    "nombre_archivo = 'review.json'\n",
    "nombre = nombre_archivo.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado Etapa 1: 34/34  100.0%\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "cont_max = 34\n",
    "Tamano = 200000\n",
    "review = pd.DataFrame()\n",
    "for chunk in pd.read_json(path_incial+nombre_archivo,chunksize=Tamano, lines=True):\n",
    "    # Visualizacion de avance\n",
    "    clear_output(wait=True)\n",
    "    print('Completado Etapa 1: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "        \n",
    "    #Armado de archivo\n",
    "    if cont == 0:\n",
    "        chunk.to_json(path_final+nombre+'_inicial.json',orient=\"records\",lines=True)\n",
    "    elif cont < 30:\n",
    "        chunk.to_json(path_final+nombre+'_incremental_'+str(cont)+'.json',orient=\"records\",lines=True)\n",
    "    else:\n",
    "        df = pd.read_json(path_final+nombre+'_incremental_'+str(np.random.randint(1,30))+'.json',lines=True)\n",
    "        review = pd.concat([df.sample(20000), chunk], ignore_index=True)\n",
    "        review.to_json(path_final+nombre+'_incremental_con_repeticiones'+str(cont)+'.json',orient=\"records\",lines=True)\n",
    "    cont = cont + 1\n",
    "    \n",
    "#tiempo aprox = 7 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_aleatorio2.json\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "df.to_json(path_final+nombre+'_aleatorio1.json',lines=True,orient=\"records\")\n",
    "\n",
    "columnas = ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny','cool', 'text', 'date']\n",
    "df = pd.DataFrame(np.random.randn(1000, len(columnas)), columns=columnas)\n",
    "df.to_json(path_final+nombre+'_aleatorio2.json',lines=True,orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Creacion datos de prueba bussines y user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_inicial = pd.read_json(path_final+nombre+'_inicial.json',lines=True)\n",
    "user_id = pd.DataFrame(data = review_inicial['user_id'].unique(),columns = ['user_id'])\n",
    "bussines_id = pd.DataFrame(data = review_inicial['business_id'].unique(),columns = ['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Creacion de datos de prueba user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = 'user.json'\n",
    "nombre = nombre_archivo.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado : 9/9  100.0%\n"
     ]
    }
   ],
   "source": [
    "user_inicial = pd.DataFrame()\n",
    "cont = 0\n",
    "cont_max = 9\n",
    "Tamano = 200000\n",
    "for chunk in pd.read_json(path_incial+nombre_archivo,chunksize=Tamano, lines=True):\n",
    "     # Visualizacion de avance\n",
    "    clear_output(wait=True)\n",
    "    print('Completado : '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    #Se verifica que filas del chunk de la tabla user tienen codigo 'user_id' en review_inicial\n",
    "    df_chunk_user_id =  pd.merge(user_id,chunk,on = 'user_id',how='outer',indicator=True)\n",
    "\n",
    "    #Se agrega a la tabla \"user_inicial\" cualquier fila que este presente en review_inicial\n",
    "    user_inicial_chunk = df_chunk_user_id[df_chunk_user_id['_merge']=='both'].drop(['_merge'], axis=1)\n",
    "    user_inicial = pd.concat([user_inicial, user_inicial_chunk], ignore_index=True) \n",
    "    \n",
    "    #Nuevas filas de user se cargaran de forma incremental\n",
    "    user_incremental_chunk = df_chunk_user_id[df_chunk_user_id['_merge']=='right_only'].drop(['_merge'], axis=1)\n",
    "    if cont < 8:\n",
    "        user_incremental_chunk.to_json(path_final+nombre+'_incremental_'+str(cont)+'.json',orient=\"records\",lines=True)\n",
    "    else:\n",
    "        user_incremental_repetidos =  pd.concat([user_inicial.sample(user_inicial.shape[0]//5), user_incremental_chunk], ignore_index=True) \n",
    "        user_incremental_repetidos.to_json(path_final+nombre+'_incremental_con_repeticiones'+str(cont)+'.json',orient=\"records\",lines=True) \n",
    "        pass\n",
    "    cont = cont + 1\n",
    "\n",
    "user_inicial.to_json(path_final+nombre+'_inicial.json',orient=\"records\",lines=True)\n",
    "\n",
    "#tiempo aprox = 4 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "df.to_json(path_final+nombre+'_aleatorio1.json',lines=True,orient=\"records\")\n",
    "\n",
    "columnas = ['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n",
    "       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n",
    "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "       'compliment_photos']\n",
    "df = pd.DataFrame(np.random.randn(1000, len(columnas)), columns=columnas)\n",
    "df.to_json(path_final+nombre+'_aleatorio2.json',lines=True,orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creacion de datos de prueba bussines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = 'business.json'\n",
    "nombre = nombre_archivo.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = pd.read_json(path_incial+nombre_archivo,lines=True)\n",
    "\n",
    "#Se verifica que filas del chunk de la tabla user tienen codigo 'user_id' en review_inicial\n",
    "df_business =  pd.merge(bussines_id,business,on = 'business_id',how='outer',indicator=True)\n",
    "\n",
    "business_inicial = df_business[df_business['_merge']=='both'].drop(['_merge'], axis=1)\n",
    "business_inicial.to_json(path_final+nombre+'_inicial.json',orient=\"records\",lines=True)\n",
    "\n",
    "\n",
    "business_incremental = df_business[df_business['_merge']=='right_only'].drop(['_merge'], axis=1)\n",
    "\n",
    "business_incremental1 = business_incremental.iloc[0:business_incremental.shape[0]//2,:]\n",
    "business_incremental1.to_json(path_final+nombre+'_incremental.json',orient=\"records\",lines=True)\n",
    "\n",
    "business_incremental2 = business_incremental.iloc[business_incremental.shape[0]//2+1:,:]\n",
    "repetidos  = business_inicial.sample(business_inicial.shape[0]//4)\n",
    "repetidos  = pd.concat([business_incremental2, repetidos], ignore_index=True)\n",
    "repetidos.to_json(path_final+nombre+'_incremental_con_repeticiones.json',orient=\"records\",lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "df.to_json(path_final+nombre+'_aleatorio1.json',lines=True,orient=\"records\")\n",
    "\n",
    "columnas = ['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
    "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
    "       'attributes', 'categories', 'hours']\n",
    "df = pd.DataFrame(np.random.randn(1000, len(columnas)), columns=columnas)\n",
    "df.to_json(path_final+nombre+'_aleatorio2.json',lines=True,orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a5f5e9430e63759269e8e709c4002b1ad533978ca32d2fcf985e534411cec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
